{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are extremely fast and effective with large datasets with high dimensionality. Naive Bayes models have made a name for themselves because they are often quick and dirty. Usually there are few tunable parameters and so they make for a great baseline starter classification algorithm to throw at a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes takes advantage of the Bayesian Theorem, it describes the relationship of conditional probabilities of statistical quantities. In short it means that it works to find the probability of a label, given a set of input features. This can be written as $ P\\left( L\\ |\\ features \\right) $ -> *Probability of L (label) given features*. This will give the probability chance that $L$ is the class label for the input with $features$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of finding quantities the Bayesian Theorem espresses this as \n",
    "\n",
    "$$ P\\left(L\\ |\\ features\\right) = \n",
    "\\frac{P\\left(L\\ |\\ features\\right)P\\left(L\\right)}{P\\left(features\\right)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to determine between two different class labels ($L_1$ and $L_2$) we make a ratio of the probabilities of each label, given the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{P(L_1 | features)}{P(L_2 | features)} = \\frac{P(features | L_1)}{P(features | L_2} \\frac{P(L_1)}{P(L_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put these probability computations to work we need a model to compute the probabilities of membership for each label on a set of features, $P(features | L_i)$. Such a model is called a generative model because it describes the hypothetical generative process for which the data came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This itself is a very difficult task however this is where the *naive* in naive Bayes comes from. If we make very naive assumptions about the generative process for each label then those predictions can come together to make a (very loose) approximation for each class. Now the Bayesian Classification can continue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simpler classifiers is the Gaussian naive Bayes. This assumes that the data from each is sourced from a simple Gaussian distribution (normal bell curve distrubution). It will find the mean and standard deviation of each label, which is shown with the rings surrounding both groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/gaussian-NB.png\" alt=\"guassian naive bayes grouping\" style=\"width: 75%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data[:5])\n",
    "print(iris.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150  points : 6\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred =clf.predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total\", \n",
    "      iris.data.shape[0], \n",
    "      \" points :\", \n",
    "      (iris.target != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classification algorithms strong assumptions about the data. They will work tremendously well when the data happens to fit those assumptions, other times they will not perform with the likes of a more complicated model.\n",
    "\n",
    "Useful for\n",
    "- High dimensional data\n",
    "- Speed for training, prediction, development (no params)\n",
    "- Baseline performance marks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
